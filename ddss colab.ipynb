{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejBrgFai5oQl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "162c4fa0-460e-4ee9-c63b-d30247c1d39e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7d6010e29a52>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Create keyspace keyspace1 with replication = { 'class':'SimpleStratergy','replication_factor': '3' };\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#Creating Data Model using Cassandra.\n",
        "Create keyspace keyspace1 with replication = { 'class':'SimpleStratergy','replication_factor': '3' };\n",
        "Create table dept ( dept_id int PRIMARY KEY, dept_name text, dept_loc text);\n",
        "Create table emp ( emp_id int PRIMARY KEY, emp_name text, dept_id int,email text, phone text );\n",
        "Insert Queries for dept table\n",
        "Insert into dept (dept_id, dept_name, dept_loc) values (1001,'Accounts', 'Mumbai');\n",
        "Insert into dept (dept_id, dept_name, dept_loc) values (1002,'Marketing', 'Delhi');\n",
        "Insert into dept (dept_id, dept_name, dept_loc) values (1003, 'HR','Chennai');\n",
        "Insert Queries for emp table\n",
        "Insert into emp ( emp_id, emp_name, dept_id, email, phone ) values\n",
        "(1001, 'ABCD',1001, 'abcd@company.com', '1122334455');\n",
        "Insert into emp ( emp_id, emp_name, dept_id, email, phone ) values\n",
        "(1002, 'DEFG',1001, 'defg@company.com', '2233445566');\n",
        "Insert into emp ( emp_id, emp_name, dept_id, email, phone ) values\n",
        "(1003, 'GHIJ',1002, 'ghij@company.com', '3344556677');\n",
        "Insert into emp ( emp_id, emp_name, dept_id, email, phone ) values\n",
        "(1004, 'JKLM',1002, 'jklm@company.com', '4455667788');\n",
        "Insert into emp ( emp_id, emp_name, dept_id, email, phone ) values\n",
        "(1005, 'MNOP',1003, 'mnop@company.com', '5566778899');\n",
        "Insert into emp ( emp_id, emp_name, dept_id, email, phone ) values\n",
        "(1006, 'MNOP',1003, 'mnop@company.com', '5566778844');\n",
        "Select-all Query for emp table\n",
        "select * from emp\n",
        "Select-all Query for dept table\n",
        "select * from dept\n",
        "Update query for dept table where dept_id=1003 set dept_name is Human Resource\n",
        "update dept set dept_name='Human Resource' where dept_id=1003;\n",
        "Delete query for emp table where emp_id=1006 and Select-all from emp\n",
        "delete from emp where emp_id=1006;\n",
        "select * from emp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2A. Text delimited CSV to HORUS format\n",
        "import pandas as pd\n",
        "sInputFileName = \"/content/sample_data/country_code.csv\"\n",
        "InputData = pd.read_csv(sInputFileName, encoding=\"latin-1\")\n",
        "print(\"Input Data Values ===================================\")\n",
        "print(InputData)\n",
        "print(\"=====================================================\")\n",
        "ProcessData = InputData\n",
        "ProcessData.drop(\"ISO-2-CODE\", axis=1, inplace=True)\n",
        "ProcessData.drop(\"ISO-3-Code\", axis=1, inplace=True)\n",
        "ProcessData.rename(columns={\"Country\": \"CountryName\"}, inplace=True)\n",
        "ProcessData.rename(columns={\"ISO-M49\": \"CountryNumber\"}, inplace=True)\n",
        "ProcessData.set_index(\"CountryNumber\", inplace=True)\n",
        "ProcessData.sort_values(\"CountryName\", axis=0, ascending=False,\n",
        "inplace=True)\n",
        "print(\"Process Data Values =================================\")\n",
        "print(ProcessData)\n",
        "print(\"=====================================================\")\n",
        "OutputData = ProcessData\n",
        "sOutputFileName = \"HORUS-CSV-Country.csv\"\n",
        "OutputData.to_csv(sOutputFileName, index=False)\n",
        "print(\"CSV to HORUS - Done\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7w8Ywls6oea",
        "outputId": "56169af1-c471-479b-b5d8-a2392808d7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data Values ===================================\n",
            "                       Country ISO-2-CODE ISO-3-Code  ISO-M49\n",
            "0                  Afghanistan         AF        AFG        4\n",
            "1                Aland Islands         AX        ALA      248\n",
            "2                      Albania         AL        ALB        8\n",
            "3                      Algeria         DZ        DZA       12\n",
            "4               American Samoa         AS        ASM       16\n",
            "..                         ...        ...        ...      ...\n",
            "242  Wallis and Futuna Islands         WF        WLF      876\n",
            "243             Western Sahara         EH        ESH      732\n",
            "244                      Yemen         YE        YEM      887\n",
            "245                     Zambia         ZM        ZMB      894\n",
            "246                   Zimbabwe         ZW        ZWE      716\n",
            "\n",
            "[247 rows x 4 columns]\n",
            "=====================================================\n",
            "Process Data Values =================================\n",
            "                             CountryName\n",
            "CountryNumber                           \n",
            "716                             Zimbabwe\n",
            "894                               Zambia\n",
            "887                                Yemen\n",
            "732                       Western Sahara\n",
            "876            Wallis and Futuna Islands\n",
            "...                                  ...\n",
            "16                        American Samoa\n",
            "12                               Algeria\n",
            "8                                Albania\n",
            "248                        Aland Islands\n",
            "4                            Afghanistan\n",
            "\n",
            "[247 rows x 1 columns]\n",
            "=====================================================\n",
            "CSV to HORUS - Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#xml to horus\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "def df2xml(data):\n",
        "  header = data.columns\n",
        "  root = ET.Element(\"root\")\n",
        "  for row in range(data.shape[0]):\n",
        "    entry = ET.SubElement(root, \"entry\")\n",
        "    for index in range(data.shape[1]):\n",
        "      schild = str(header[index])\n",
        "      child = ET.SubElement(entry, schild)\n",
        "    if str(data[schild][row]) != \"nan\":\n",
        "      child.text = str(data[schild][row])\n",
        "    else:\n",
        "      child.text = \"n/a\"\n",
        "      entry.append(child)\n",
        "      result = ET.tostring(root)\n",
        "    return result\n",
        "def xml2df(xml_data):\n",
        "  root = ET.XML(xml_data)\n",
        "  all_records = []\n",
        "  for i, child in enumerate(root):\n",
        "    record = {}\n",
        "  for subchild in child:\n",
        "    record[subchild.tag] = subchild.text\n",
        "    all_records.append(record)\n",
        "  return pd.DataFrame(all_records)\n",
        "sInputFileName = \"/content/sample_data/country_code.csv\"\n",
        "InputData = open(sInputFileName).read()\n",
        "print(\"=====================================================\")\n",
        "print(\"Input Data Values ===================================\")\n",
        "print(\"=====================================================\")\n",
        "print(\"=====================================================\")\n",
        "ProcessDataXML = InputData\n",
        "ProcessData = xml2df(ProcessDataXML)\n",
        "ProcessData.drop(\"ISO-2-CODE\", axis=1, inplace=True)\n",
        "ProcessData.drop(\"ISO-3-Code\", axis=1, inplace=True)\n",
        "ProcessData.rename(columns={\"Country\": \"CountryName\"}, inplace=True)\n",
        "ProcessData.rename(columns={\"ISO-M49\": \"CountryNumber\"}, inplace=True)\n",
        "ProcessData.set_index(\"CountryNumber\", inplace=True)\n",
        "ProcessData.sort_values(\"CountryName\", axis=0, ascending=False,inplace=True)\n",
        "print(\"=====================================================\")\n",
        "print(\"Process Data Values =================================\")\n",
        "print(\"=====================================================\")\n",
        "print(ProcessData)\n",
        "print(\"=====================================================\")\n",
        "OutputData = ProcessData\n",
        "sOutputFileName = \"HORUS-XML-Country.csv\"\n",
        "OutputData.to_csv(sOutputFileName, index=False)\n",
        "print(\"=====================================================\")\n",
        "print(\"XML to HORUS - Done\")\n",
        "print(\"=====================================================\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fn5PvEJz7LvJ",
        "outputId": "979c1877-65e1-40ac-fb7a-ad847b3e4c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2fde10cb8ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0msInputFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/sample_data/country_code.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mInputData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msInputFileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=====================================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input Data Values ===================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x98 in position 1238: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# json to horus\n",
        "import pandas as pd\n",
        "sInputFileName = \"/content/sample_data/Country_Code.json\"\n",
        "InputData = pd.read_json(sInputFileName, orient=\"index\",\n",
        "encoding=\"latin-1\")\n",
        "print(\"Input Data Values ===================================\")\n",
        "print(InputData)\n",
        "print(\"=====================================================\")\n",
        "ProcessData = InputData\n",
        "ProcessData.drop(\"ISO-2-CODE\", axis=1, inplace=True)\n",
        "ProcessData.drop(\"ISO-3-Code\", axis=1, inplace=True)\n",
        "ProcessData.rename(columns={\"Country\": \"CountryName\"}, inplace=True)\n",
        "ProcessData.rename(columns={\"ISO-M49\": \"CountryNumber\"}, inplace=True)\n",
        "ProcessData.set_index(\"CountryNumber\", inplace=True)\n",
        "ProcessData.sort_values(\"CountryName\", axis=0, ascending=False,\n",
        "inplace=True)\n",
        "print(\"Process Data Values =================================\")\n",
        "print(ProcessData)\n",
        "print(\"=====================================================\")\n",
        "OutputData = ProcessData\n",
        "sOutputFileName = \"HORUS-JSON-Country.csv\"\n",
        "OutputData.to_csv(sOutputFileName, index=False)\n",
        "print(\"JSON to HORUS - Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntwpy6WoBpd4",
        "outputId": "ca9d2af2-c4d9-4e11-b9e4-553a691c4e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data Values ===================================\n",
            "                       Country ISO-2-CODE ISO-3-Code  ISO-M49\n",
            "0                  Afghanistan         AF        AFG        4\n",
            "1                Aland Islands         AX        ALA      248\n",
            "2                      Albania         AL        ALB        8\n",
            "3                      Algeria         DZ        DZA       12\n",
            "4               American Samoa         AS        ASM       16\n",
            "..                         ...        ...        ...      ...\n",
            "242  Wallis and Futuna Islands         WF        WLF      876\n",
            "243             Western Sahara         EH        ESH      732\n",
            "244                      Yemen         YE        YEM      887\n",
            "245                     Zambia         ZM        ZMB      894\n",
            "246                   Zimbabwe         ZW        ZWE      716\n",
            "\n",
            "[247 rows x 4 columns]\n",
            "=====================================================\n",
            "Process Data Values =================================\n",
            "                             CountryName\n",
            "CountryNumber                           \n",
            "716                             Zimbabwe\n",
            "894                               Zambia\n",
            "887                                Yemen\n",
            "732                       Western Sahara\n",
            "876            Wallis and Futuna Islands\n",
            "...                                  ...\n",
            "16                        American Samoa\n",
            "12                               Algeria\n",
            "8                                Albania\n",
            "248                        Aland Islands\n",
            "4                            Afghanistan\n",
            "\n",
            "[247 rows x 1 columns]\n",
            "=====================================================\n",
            "JSON to HORUS - Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mysql to horus\n",
        "import pandas as pd\n",
        "import sqlite3 as sq\n",
        "sInputFileName = \"/content/sample_data/utility.db\" \n",
        "sInputTable = \"Country_Code\"\n",
        "conn = sq.connect(sInputFileName)\n",
        "sSQL = \"select * FROM \" + sInputTable + \";\"\n",
        "InputData=pd.read_sql_query(sSQL, conn)\n",
        "print(\"Input Data Values ===================================\")\n",
        "print(InputData)\n",
        "print(\"=====================================================\")\n",
        "ProcessData = InputData\n",
        "ProcessData.drop(\"ISO-2-CODE\", axis=1, inplace=True)\n",
        "ProcessData.drop(\"ISO-3-Code\", axis=1, inplace=True)\n",
        "ProcessData.rename(columns={\"Country\": \"CountryName\"}, inplace=True)\n",
        "ProcessData.rename(columns={\"ISO-M49\": \"CountryNumber\"}, inplace=True)\n",
        "ProcessData.set_index(\"CountryNumber\", inplace=True)\n",
        "ProcessData.sort_values(\"CountryName\", axis=0, ascending=False,\n",
        "inplace=True)\n",
        "print(\"Process Data Values =================================\")\n",
        "print(ProcessData)\n",
        "print(\"=====================================================\")\n",
        "OutputData = ProcessData\n",
        "sOutputFileName = \"HORUS-CSV-Country.csv\"\n",
        "OutputData.to_csv(sOutputFileName, index=False)\n",
        "print(\"Database to HORUS - Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n49YriR3CMgC",
        "outputId": "c1af4a3c-d674-484b-b0ec-2f5eede7ea0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data Values ===================================\n",
            "     index                    Country ISO-2-CODE ISO-3-Code  ISO-M49\n",
            "0        0                Afghanistan         AF        AFG        4\n",
            "1        1              Aland Islands         AX        ALA      248\n",
            "2        2                    Albania         AL        ALB        8\n",
            "3        3                    Algeria         DZ        DZA       12\n",
            "4        4             American Samoa         AS        ASM       16\n",
            "..     ...                        ...        ...        ...      ...\n",
            "242    242  Wallis and Futuna Islands         WF        WLF      876\n",
            "243    243             Western Sahara         EH        ESH      732\n",
            "244    244                      Yemen         YE        YEM      887\n",
            "245    245                     Zambia         ZM        ZMB      894\n",
            "246    246                   Zimbabwe         ZW        ZWE      716\n",
            "\n",
            "[247 rows x 5 columns]\n",
            "=====================================================\n",
            "Process Data Values =================================\n",
            "               index                CountryName\n",
            "CountryNumber                                  \n",
            "716              246                   Zimbabwe\n",
            "894              245                     Zambia\n",
            "887              244                      Yemen\n",
            "732              243             Western Sahara\n",
            "876              242  Wallis and Futuna Islands\n",
            "...              ...                        ...\n",
            "16                 4             American Samoa\n",
            "12                 3                    Algeria\n",
            "8                  2                    Albania\n",
            "248                1              Aland Islands\n",
            "4                  0                Afghanistan\n",
            "\n",
            "[247 rows x 2 columns]\n",
            "=====================================================\n",
            "Database to HORUS - Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#practical 3 utilities and auditing\n",
        "# 3A fixers utilities\n",
        "import string\n",
        "import datetime as dt\n",
        "print(\"\\n#1 Removing leading or lagging spaces from a data entry\")\n",
        "baddata = \" Data Science with too many spaces is bad!!! \"\n",
        "print(\">\", baddata, \"<\")\n",
        "cleandata = baddata.strip()\n",
        "print(\">\", cleandata, \"<\")\n",
        "print(\"#2 Removing nonprintable characters from a data entry\")\n",
        "printable = set(string.printable)\n",
        "baddata = \"Data\\x00Science with\\x02 funny characters is \\x10bad!!!\"\n",
        "cleandata = \"\".join(filter(lambda x: x in string.printable, baddata))\n",
        "print(\"Bad Data : \", baddata)\n",
        "print(\"Clean Data : \", cleandata)\n",
        "print(\"# 3 Reformatting data entry to match specific formatting criteria.\")\n",
        "baddate = dt.date(2019, 10, 31)\n",
        "baddata = format(baddate, \"%Y-%m-%d\")\n",
        "gooddate = dt.datetime.strptime(baddata, \"%Y-%m-%d\")\n",
        "gooddata = format(gooddate, \"%d %B %Y\")\n",
        "print(\"Bad Data : \", baddata)\n",
        "print(\"Good Data : \", gooddata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDrJ32CkC0yo",
        "outputId": "d5c30511-22f4-4770-f3d9-f557610a0d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#1 Removing leading or lagging spaces from a data entry\n",
            ">  Data Science with too many spaces is bad!!!  <\n",
            "> Data Science with too many spaces is bad!!! <\n",
            "#2 Removing nonprintable characters from a data entry\n",
            "Bad Data :  Data\u0000Science with\u0002 funny characters is \u0010bad!!!\n",
            "Clean Data :  DataScience with funny characters is bad!!!\n",
            "# 3 Reformatting data entry to match specific formatting criteria.\n",
            "Bad Data :  2019-10-31\n",
            "Good Data :  31 October 2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3B. Data Binning or Bucketing\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "np.random.seed(0)\n",
        "mu = 90 \n",
        "sigma = 25 \n",
        "x = mu + sigma * np.random.randn(5000)\n",
        "num_bins = 25\n",
        "fig, ax = plt.subplots()\n",
        "n, bins, patches = ax.hist(\n",
        "x,\n",
        "num_bins,\n",
        "color=\"royalblue\",\n",
        "density=1,\n",
        "alpha=1,\n",
        "align=\"right\",\n",
        "orientation=\"vertical\",\n",
        "cumulative=True,\n",
        ")\n",
        "y = stats.norm.pdf(bins, mu, sigma)\n",
        "ax.plot(bins, y, \"--\")\n",
        "ax.set_xlabel(\"Example Data\")\n",
        "ax.set_ylabel(\"Probability density\")\n",
        "sTitle = (\n",
        "r\"Histogram \"\n",
        "+ str(len(x))\n",
        "+ \" entries into \"\n",
        "+ str(num_bins)\n",
        "+ \" Bins: $\\mu=\"\n",
        "+ str(mu)\n",
        "+ \"$, $\\sigma=\"\n",
        "+ str(sigma)\n",
        "+ \"$\"\n",
        ")\n",
        "ax.set_title(sTitle)\n",
        "fig.tight_layout()\n",
        "sPathFig = \"DU-Histogram.png\"\n",
        "fig.savefig(sPathFig)\n",
        "plt.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "JgYKXem-DK2H",
        "outputId": "ddc7f8c4-72dc-4935-91d5-326d5b5783ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3C Averaging of data\n",
        "import pandas as pd\n",
        "InputFileName = \"IP_DATA_CORE.csv\"\n",
        "OutputFileName = \"Retrieve_Router_Location.csv\"\n",
        "Base = \"VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \")\n",
        "print(\"################################\")\n",
        "sFileName = Base + \"/01-Vermeulen/00-RawData/\" + InputFileName \n",
        "print(\"Loading :\", sFileName)\n",
        "IP_DATA_ALL = pd.read_csv(sFileName,header=0,low_memory=False,usecols=[\"Country\", \"Place Name\", \"Latitude\", \"Longitude\"],encoding=\"latin-1\",)\n",
        "IP_DATA_ALL.rename(columns={\"Place Name\": \"Place_Name\"}, inplace=True)\n",
        "AllData = IP_DATA_ALL[[\"Country\", \"Place_Name\", \"Latitude\"]]\n",
        "print(AllData)\n",
        "MeanData = AllData.groupby([\"Country\",\"Place_Name\"])[\"Latitude\"].mean()\n",
        "print(MeanData)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "YRwzceO-DgOz",
        "outputId": "6e7b1a0e-10a4-4a4e-c655-97b6dd66d02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################\n",
            "Working Base : VKHCG  using \n",
            "################################\n",
            "Loading : VKHCG/01-Vermeulen/00-RawData/IP_DATA_CORE.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-453de9733a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/01-Vermeulen/00-RawData/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mInputFileName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msFileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mIP_DATA_ALL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msFileName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Place Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Latitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Longitude\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mIP_DATA_ALL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Place Name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Place_Name\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mAllData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIP_DATA_ALL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Place_Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Latitude\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VKHCG/01-Vermeulen/00-RawData/IP_DATA_CORE.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#practical 4\n",
        "#4A data processing using R\n",
        "library(readr)\n",
        "IP_DATA_ALL <- read_csv(\"D:/VKHCG/01-Vermeulen/00-RawData/IP_DATA_ALL.csv\")\n",
        "View(IP_DATA_ALL)\n",
        "spec(IP_DATA_ALL)\n",
        "library(tibble)\n",
        "set_tidy_names(IP_DATA_ALL, syntactic = TRUE, quiet = FALSE)\n",
        "sapply(IP_DATA_ALL_FIX, typeof)\n",
        "library(data.table)\n",
        "hist_country = data.table(Country = unique(IP_DATA_ALL_FIX[is.na(IP_DATA_ALL_FIX['Country']) == 0,] $Country))\n",
        "setorder(hist_country, 'Country')\n",
        "hist_country_with_id = rowid_to_column(hist_country, var =\"RowIDCountry\")\n",
        "View(hist_country_fix)\n",
        "IP_DATA_COUNTRY_FREQ = data.table(with(IP_DATA_ALL_FIX,table(Country)))\n",
        "View(IP_DATA_COUNTRY_FREQ)\n",
        "hist_latitude = data.table(Latitude =\n",
        "unique(IP_DATA_ALL_FIX[is.na(IP_DATA_ALL_with_ID['Latitude']) ==0,]$Latitude))\n",
        "setkeyv(hist_latitude, 'Latitude')\n",
        "setorder(hist_latitude)\n",
        "hist_latitude_with_id = rowid_to_column(hist_latitude, var = \"RowID\")\n",
        "View(hist_latitude_with_id)\n",
        "IP_DATA_Latitude_FREQ = data.table(with(IP_DATA_ALL_FIX, table(Latitude)))\n",
        "View(IP_DATA_Latitude_FREQ)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], min, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Country'], min, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], max, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Country'], max, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], mean, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], median, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], range, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], quantile, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Latitude'], sd, na.rm = TRUE)\n",
        "sapply(IP_DATA_ALL_FIX[, 'Longitude'], sd, na.rm = TRUE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "tC1a4u6jFLUo",
        "outputId": "8ddcd4be-a472-4fc9-bdee-d1f3b740c285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-4164630500ec>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    hist_country = data.table(Country = unique(IP_DATA_ALL_FIX[is.na(IP_DATA_ALL_FIX['Country']) == 0,] $Country))\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4B Program to retrieve different attributes of data\n",
        "import os\n",
        "import pandas as pd\n",
        "Base = \"D:/VKHCG\"\n",
        "sFileName = Base + \"/01-Vermeulen/00-RawData/IP_DATA_ALL.csv\"\n",
        "print(\"Loading:\", sFileName)\n",
        "IP_DATA_ALL = pd.read_csv(sFileName, header=0, low_memory=False, encoding=\"latin-1\")\n",
        "sFileDir = Base + \"01-Vermeulen/01-Retrieve/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):os.makedirs(sFileDir)\n",
        "print(\"Rows\", IP_DATA_ALL.shape[0])\n",
        "print(\"Columns\", IP_DATA_ALL.shape[1])\n",
        "print(\"### Raw Data Set #####################################\")\n",
        "for i in range(0, len(IP_DATA_ALL.columns)):\n",
        "print(IP_DATA_ALL.columns[i], type(IP_DATA_ALL.columns[i]))\n",
        "print(\"### Fixed Data Set #####################################\")\n",
        "IP_DATA_ALL_FIX = IP_DATA_ALL\n",
        "for i in range(0, len(IP_DATA_ALL.columns)):\n",
        "cNameOld = IP_DATA_ALL_FIX.columns[i] + \"\"\n",
        "cNameNew = cNameOld.strip().replace(\" \", \".\")\n",
        "IP_DATA_ALL_FIX.columns.values[i] = cNameNew\n",
        "print(IP_DATA_ALL.columns[i], type(IP_DATA_ALL.columns[i]))\n",
        "print(\"Fixed Data Set with ID\")\n",
        "IP_DATA_ALL_with_ID = IP_DATA_ALL_FIX\n",
        "IP_DATA_ALL_with_ID.index.names = [\"RowID\"]\n",
        "sFileName2 = sFileDir + \"/Retrieve_IP_DATA.csv\"\n",
        "IP_DATA_ALL_with_ID.to_csv(sFileName2, index=True, encoding=\"latin-1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "jyPBd_LjGAdA",
        "outputId": "ff1ba527-8f0a-42d5-fc66-485dc44e5576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-71c3477b84d5>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    print(IP_DATA_ALL.columns[i], type(IP_DATA_ALL.columns[i]))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4C.Data Pattern\n",
        "\n",
        "library(readr)\n",
        "library(data.table)\n",
        "FileName = paste0('C:/VKHCG/01-Vermeulen/00-RawData/IP_DATA_ALL.csv')\n",
        "IP_DATA_ALL <- read.csv(FileName)\n",
        "hist_country = data.table(Country = unique(IP_DATA_ALL$Country))\n",
        "pattern_country = data.table(Country = hist_country$Country,\n",
        "PatternCountry = hist_country$Country)\n",
        "oldchar = c(letters, LETTERS)\n",
        "newchar = replicate(length(oldchar), \"A\")\n",
        "for (r in seq(nrow(pattern_country))) {\n",
        "s = pattern_country[r,]$PatternCountry\n",
        "for (c in seq(length(oldchar))) {\n",
        "s = chartr(oldchar[c], newchar[c], s)\n",
        "}\n",
        "s = chartr(\" \", \"b\", s)\n",
        "s = chartr(\".\", \"u\", s)\n",
        "pattern_country[r,]$PatternCountry = s\n",
        "}\n",
        "View(pattern_country)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "DTh2Ur-QGY7U",
        "outputId": "d1080d9f-1884-48fd-cdff-e3c717f98db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-588fc330f896>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    hist_country = data.table(Country = unique(IP_DATA_ALL$Country))\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#practical 5\n",
        "# 5A. Perform error management on the given data using pandas package\n",
        "#I. Drop the Columns Where All Elements Are Missing Values\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "Base = \"c:/VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "sInputFileName = \"Good-or-Bad.csv\"\n",
        "sOutputFileName = \"Good-or-Bad-01.csv\"\n",
        "Company = \"01-Vermeulen\"\n",
        "Base = \"VKHCG\"\n",
        "sFileDir = Base + \"/\" + Company + \"/02-Assess/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):os.makedirs(sFileDir)\n",
        "sFileName = Base + \"/\" + Company + \"/00-RawData/\" + sInputFileName\n",
        "print(\"Loading :\", sFileName)\n",
        "RawData = pd.read_csv(sFileName, header=0)\n",
        "print(\"################################\")\n",
        "print(\"## Raw Data Values\")\n",
        "print(\"################################\")\n",
        "print(RawData)\n",
        "print(\"################################\")\n",
        "print(\"## Data Profile\")\n",
        "print(\"################################\")\n",
        "print(\"Rows :\", RawData.shape[0])\n",
        "print(\"Columns :\", RawData.shape[1])\n",
        "print(\"################################\")\n",
        "sFileName = sFileDir + \"/\" + sInputFileName\n",
        "RawData.to_csv(sFileName, index=False)\n",
        "TestData = RawData.dropna(axis=1, how=\"all\")\n",
        "print(\"################################\")\n",
        "print(\"## Test Data Values\")\n",
        "print(\"################################\")\n",
        "print(TestData)\n",
        "print(\"################################\")\n",
        "print(\"## Data Profile\")\n",
        "print(\"################################\")\n",
        "print(\"Rows :\", TestData.shape[0])\n",
        "print(\"Columns :\", TestData.shape[1])\n",
        "print(\"################################\")\n",
        "sFileName = sFileDir + \"/\" + sOutputFileName\n",
        "TestData.to_csv(sFileName, index=False)\n",
        "print(\"################################\")\n",
        "print(\"### Done!! #####################\")\n",
        "print(\"################################\")\n",
        "\n",
        "#II. Drop the Columns Where Any of the Elements Is Missing Values\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "Base = \"VKHCG\"\n",
        "sInputFileName = \"Good-or-Bad.csv\"\n",
        "sOutputFileName = \"Good-or-Bad-02.csv\"\n",
        "Company = \"01-Vermeulen\"\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "sFileDir = Base + \"/\" + Company + \"/02-Assess/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):\n",
        "  os.makedirs(sFileDir)\n",
        "sFileName = Base + \"/\" + Company + \"/00-RawData/\" + sInputFileName\n",
        "print(\"Loading :\", sFileName)\n",
        "RawData = pd.read_csv(sFileName, header=0)\n",
        "print(\"## Raw Data Values\")\n",
        "print(RawData)\n",
        "print(\"## Data Profile\")\n",
        "print(\"Rows :\", RawData.shape[0])\n",
        "print(\"Columns :\", RawData.shape[1])\n",
        "sFileName = sFileDir + \"/\" + sInputFileName\n",
        "RawData.to_csv(sFileName, index=False)\n",
        "TestData = RawData.dropna(axis=1, how=\"any\")\n",
        "print(\"## Test Data Values\")\n",
        "print(TestData)\n",
        "print(\"## Data Profile\")\n",
        "print(\"Rows :\", TestData.shape[0])\n",
        "print(\"Columns :\", TestData.shape[1])\n",
        "sFileName = sFileDir + \"/\" + sOutputFileName\n",
        "TestData.to_csv(sFileName, index=False)\n",
        "\n",
        "#III. Keep Only the Rows That Contain a Maximum of Two Missing Values\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "sInputFileName = \"Good-or-Bad.csv\"\n",
        "sOutputFileName = \"Good-or-Bad-03.csv\"\n",
        "Company = \"01-Vermeulen\"\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, sys.platform)\n",
        "sFileDir = Base + \"/\" + Company + \"/02-Assess/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):\n",
        "  os.makedirs(sFileDir)\n",
        "### Import Warehouse\n",
        "sFileName = Base + \"/\" + Company + \"/00-RawData/\" + sInputFileName\n",
        "print(\"Loading :\", sFileName)\n",
        "RawData = pd.read_csv(sFileName, header=0)\n",
        "print(\"## Raw Data Values\")\n",
        "print(RawData)\n",
        "print(\"## Data Profile\")\n",
        "print(\"Rows :\", RawData.shape[0])\n",
        "print(\"Columns :\", RawData.shape[1])\n",
        "sFileName = sFileDir + \"/\" + sInputFileName\n",
        "RawData.to_csv(sFileName, index=False)\n",
        "TestData = RawData.dropna(thresh=2)\n",
        "print(\"## Test Data Values\")\n",
        "print(TestData)\n",
        "print(\"## Data Profile\")\n",
        "print(\"Rows :\", TestData.shape[0])\n",
        "print(\"Columns :\", TestData.shape[1])\n",
        "sFileName = sFileDir + \"/\" + sOutputFileName\n",
        "TestData.to_csv(sFileName, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "5icj6c2RGkUG",
        "outputId": "a4a3f1a6-0615-4f72-bfac-7371ca07c5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################\n",
            "Working Base : c:/VKHCG  using  linux\n",
            "################################\n",
            "Loading : VKHCG/01-Vermeulen/00-RawData/Good-or-Bad.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d61f992b31d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCompany\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/00-RawData/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msInputFileName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msFileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mRawData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msFileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"################################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"## Raw Data Values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VKHCG/01-Vermeulen/00-RawData/Good-or-Bad.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5B. Write Python / R program to create the network routing diagram from the given data on routers.\n",
        "\n",
        "#assess-network-routing-company.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, sys.platform)\n",
        "sInputFileName1 = \"01-Retrieve/01-EDS/01-R/Retrieve_Country_Code.csv\"\n",
        "sInputFileName2 = \"01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv\"\n",
        "sInputFileName3 = \"01-Retrieve/01-EDS/01-R/Retrieve_IP_DATA.csv\"\n",
        "sOutputFileName = \"Assess-Network-Routing-Company.csv\"\n",
        "Company = \"01-Vermeulen\"\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sInputFileName1\n",
        "print(\"Loading :\", sFileName)\n",
        "CountryData = pd.read_csv(sFileName, header=0, low_memory=False, encoding=\"latin-1\")\n",
        "print(\"Loaded Country:\", CountryData.columns.values)\n",
        "print(\"Changed :\", CountryData.columns.values)\n",
        "CountryData.rename(columns={\"Country\": \"Country_Name\"}, inplace=True)\n",
        "CountryData.rename(columns={\"ISO-2-CODE\": \"Country_Code\"}, inplace=True)\n",
        "CountryData.drop(\"ISO-M49\", axis=1, inplace=True)\n",
        "CountryData.drop(\"ISO-3-Code\", axis=1, inplace=True)\n",
        "CountryData.drop(\"RowID\", axis=1, inplace=True)\n",
        "print(\"To :\", CountryData.columns.values)\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sInputFileName2\n",
        "print(\"Loading :\", sFileName)\n",
        "CompanyData = pd.read_csv(sFileName, header=0, low_memory=False, encoding=\"latin-1\")\n",
        "print(\"Loaded Company :\", CompanyData.columns.values)\n",
        "print(\"Changed :\", CompanyData.columns.values)\n",
        "CompanyData.rename(columns={\"Country\": \"Country_Code\"}, inplace=True)\n",
        "print(\"To :\", CompanyData.columns.values)\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sInputFileName3\n",
        "print(\"Loading :\", sFileName)\n",
        "CustomerRawData = pd.read_csv(sFileName, header=0, low_memory=False, encoding=\"latin-1\")\n",
        "print(\"Loaded Customer :\", CustomerRawData.columns.values)\n",
        "CustomerData = CustomerRawData.dropna(axis=0, how=\"any\")\n",
        "print(\"Remove Blank Country Code\")\n",
        "print(\"Reduce Rows from\", CustomerRawData.shape[0], \" to \", CustomerData.shape[0])\n",
        "print(\"Changed :\", CustomerData.columns.values)\n",
        "CustomerData.rename(columns={\"Country\": \"Country_Code\"}, inplace=True)\n",
        "print(\"To :\", CustomerData.columns.values)\n",
        "print(\"Merge Company and Country Data\")\n",
        "CompanyNetworkData = pd.merge(CompanyData, CountryData, how=\"inner\", on=\"Country_Code\")\n",
        "print(\"Change \", CompanyNetworkData.columns.values)\n",
        "for i in CompanyNetworkData.columns.values:\n",
        "  j = \"Company_\" + i\n",
        "CompanyNetworkData.rename(columns={i: j}, inplace=True)\n",
        "print(\"To \", CompanyNetworkData.columns.values)\n",
        "sFileDir = Base + \"/\" + Company + \"/02-Assess/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):\n",
        "  os.makedirs(sFileDir)\n",
        "sFileName = sFileDir + \"/\" + sOutputFileName\n",
        "print(\"Storing :\", sFileName)\n",
        "CompanyNetworkData.to_csv(sFileName, index=False, encoding=\"latin-1\")\n",
        "\n",
        "\n",
        "#assess-network-routing-customer.py\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "sInputFileName = (Base+ \"/01-Vermeulen/02-Assess/01-EDS/02-Python/Assess-Network-Routing-Company.csv\")\n",
        "sOutputFileName = \"Assess-Network-Routing-Customer.gml\"\n",
        "Company = \"01-Vermeulen\"\n",
        "sFileName = sInputFileName\n",
        "print(\"Loading :\", sFileName)\n",
        "CustomerData=pd.read_csv(sFileName, header=0, low_memory=False, encoding=\"latin-1\")\n",
        "print(\"Loaded Country:\", CustomerData.columns.values)\n",
        "print(CustomerData.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "J-Mws_csRPqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5C. Write a Python / R program to build directed acyclic graph\n",
        "\n",
        "#assess-DAG-location.py\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "sInputFileName = \"01-Retrive/01-EDS/02-Python/Retrieve_Router_Location.csv\"\n",
        "sOutputFileName1 = \"Assess-DAG-Company-Country.png\"\n",
        "sOutputFileName2 = \"Assess-DAG-Company-Country-Place.png\"\n",
        "Company = \"01-Vermeulen\"\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sInputFileName\n",
        "print(\"Loading :\", sFileName)\n",
        "CompanyData = pd.read_csv(sFileName, header=0, low_memory=False,encoding=\"latin-1\")\n",
        "print(\"Loaded Company :\", CompanyData.columns.values)\n",
        "print(CompanyData)\n",
        "print(\"Rows : \", CompanyData.shape[0])\n",
        "G1 = nx.DiGraph()\n",
        "G2 = nx.DiGraph()\n",
        "for i in range(CompanyData.shape[0]):\n",
        "  G1.add_node(CompanyData[\"Country\"][i])\n",
        "sPlaceName = CompanyData[\"Place_Name\"][i] + \"-\" + CompanyData[\"Country\"][i]\n",
        "G2.add_node(sPlaceName)\n",
        "\n",
        "for n1 in G1.nodes():\n",
        "  for n2 in G1.nodes():\n",
        "    if n1 != n2:\n",
        "      print(\"Link :\", n1, \" to \", n2)\n",
        "G1.add_edge(n1, n2)\n",
        "print(\"Nodes of graph: \")\n",
        "print(G1.nodes())\n",
        "print(\"Edges of graph: \")\n",
        "print(G1.edges())\n",
        "sFileDir = Base + \"/\" + Company + \"/02-Assess/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):\n",
        "  os.makedirs(sFileDir)\n",
        "sFileName = sFileDir + \"/\" + sOutputFileName1\n",
        "print(\"Storing :\", sFileName)\n",
        "nx.draw(\n",
        "G1,\n",
        "pos=nx.spectral_layout(G1),\n",
        "edge_color=\"g\",\n",
        "with_labels=True,\n",
        "node_size=8000,\n",
        "font_size=12,\n",
        ")\n",
        "plt.savefig(sFileName)\n",
        "plt.show() \n",
        "for n1 in G2.nodes():\n",
        "  for n2 in G2.nodes():\n",
        "   if n1 != n2:\n",
        "    print(\"Link :\", n1, \" to \", n2)\n",
        "G2.add_edge(n1, n2)\n",
        "print(\"Nodes of graph: \")\n",
        "print(G2.nodes())\n",
        "print(\"Edges of graph: \")\n",
        "print(G2.edges())\n",
        "\n",
        "sFileDir = Base + \"/\" + Company + \"/02-Assess/01-EDS/02-Python\"\n",
        "if not os.path.exists(sFileDir):\n",
        "  os.makedirs(sFileDir)\n",
        "sFileName = sFileDir + \"/\" + sOutputFileName2\n",
        "print(\"Storing :\", sFileName)\n",
        "nx.draw(\n",
        "G2,\n",
        "pos=nx.spectral_layout(G2),\n",
        "edge_color=\"b\",\n",
        "node_color=\"g\",\n",
        "with_labels=True,\n",
        "node_size=8000,\n",
        "font_size=12,\n",
        ")\n",
        "plt.savefig(sFileName) \n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "liLLCy0HS4F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#practical 6\n",
        "# 6A. Build the time hub, links, and satellites\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from pytz import timezone, all_timezones\n",
        "import pandas as pd\n",
        "import sqlite3 as sq\n",
        "from pandas.io import sql\n",
        "import uuid\n",
        "pd.options.mode.chained_assignment = None\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "Company = \"01-Vermeulen\"\n",
        "InputDir = \"00-RawData\"\n",
        "InputFileName = \"VehicleData.csv\"\n",
        "sDataBaseDir = Base + \"/\" + Company + \"/03-Process/SQLite\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "sDatabaseName = sDataBaseDir + \"/Hillman.db\"\n",
        "conn1 = sq.connect(sDatabaseName)\n",
        "sDataVaultDir = Base + \"/88-DV\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "sDatabaseName = sDataVaultDir + \"/datavault.db\"\n",
        "conn2 = sq.connect(sDatabaseName)\n",
        "base = datetime(2018, 1, 1, 0, 0, 0)\n",
        "numUnits = 10 * 365 * 24\n",
        "date_list = [base - timedelta(hours=x) for x in range(0, numUnits)]\n",
        "t = 0\n",
        "for i in date_list:\n",
        "  now_utc = i.replace(tzinfo=timezone(\"UTC\"))\n",
        "sDateTime = now_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(sDateTime)\n",
        "sDateTimeKey = sDateTime.replace(\" \", \"-\").replace(\":\", \"-\")\n",
        "t += 1\n",
        "IDNumber = str(uuid.uuid4())\n",
        "TimeLine = [\n",
        "(\"ZoneBaseKey\", [\"UTC\"]),\n",
        "(\"IDNumber\", [IDNumber]),\n",
        "(\"nDateTimeValue\", [now_utc]),\n",
        "(\"DateTimeValue\", [sDateTime]),\n",
        "(\"DateTimeKey\", [sDateTimeKey]),]\n",
        "if t == 1:\n",
        "  TimeFrame = pd.DataFrame.from_dict(dict(TimeLine))\n",
        "else:\n",
        "  TimeRow = pd.DataFrame.from_dict(dict(TimeLine))\n",
        "  TimeFrame = TimeFrame.append(TimeRow)\n",
        "  TimeHub = TimeFrame[[\"IDNumber\", \"ZoneBaseKey\", \"DateTimeKey\",\"DateTimeValue\"]]\n",
        "  TimeHubIndex = TimeHub.set_index([\"IDNumber\"], inplace=False)\n",
        "  TimeFrame.set_index([\"IDNumber\"], inplace=True)\n",
        "  sTable = \"Process-Time\"\n",
        "  print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "  TimeHubIndex.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "  sTable = \"Hub-Time\"\n",
        "  print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "  TimeHubIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "  active_timezones = all_timezones\n",
        "  z = 0\n",
        "  for zone in active_timezones:\n",
        "    t = 0\n",
        "  for j in range(TimeFrame.shape[0]):\n",
        "    now_date = TimeFrame[\"nDateTimeValue\"][j]\n",
        "    DateTimeKey = TimeFrame[\"DateTimeKey\"][j]\n",
        "    now_utc = now_date.replace(tzinfo=timezone(\"UTC\"))\n",
        "    sDateTime = now_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    now_zone = now_utc.astimezone(timezone(zone))\n",
        "    sZoneDateTime = now_zone.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(sZoneDateTime)\n",
        "    t += 1\n",
        "    z += 1\n",
        "    IDZoneNumber = str(uuid.uuid4())\n",
        "    TimeZoneLine=[(\"ZoneBaseKey\", [\"UTC\"]),(\"IDZoneNumber\", [IDZoneNumber]),(\"DateTimeKey\", [DateTimeKey]),(\"UTCDateTimeValue\", [sDateTime]),(\"Zone\", [zone]),(\"DateTimeValue\", [sZoneDateTime]),]\n",
        "    if t == 1:\n",
        "      TimeZoneFrame = pd.DataFrame.from_dict(dict(TimeZoneLine))\n",
        "    else:\n",
        "      TimeZoneRow = pd.DataFrame.from_dict(dict(TimeZoneLine))\n",
        "      TimeZoneFrame = TimeZoneFrame.append(TimeZoneRow)\n",
        "      TimeZoneFrameIndex = TimeZoneFrame.set_index([\"IDZoneNumber\"],\n",
        "      inplace=False)\n",
        "      sZone = zone.replace(\"/\", \"-\").replace(\" \", \"\")\n",
        "      sTable = \"Process-Time-\" + sZone\n",
        "      print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "      TimeZoneFrameIndex.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "      sTable = \"Satellite-Time-\" + sZone\n",
        "      print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "      TimeZoneFrameIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "      print(\"Vacuum Databases\")\n",
        "      sSQL = \"VACUUM;\"\n",
        "      sql.execute(sSQL, conn1)\n",
        "      sql.execute(sSQL, conn2)\n"
      ],
      "metadata": {
        "id": "ejE9CTUtUCBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6B. Write a program to load the vehicle data for Hillman Ltd into the data vault\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3 as sq\n",
        "from pandas.io import sql\n",
        "import uuid\n",
        "pd.options.mode.chained_assignment = None\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "Company = \"03-Hillman\"\n",
        "InputDir = \"00-RawData\"\n",
        "InputFileName = \"VehicleData.csv\"\n",
        "sDataBaseDir = Base + \"/\" + Company + \"/03-Process/SQLite\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "sDatabaseName = sDataBaseDir + \"/Hillman.db\"\n",
        "conn1 = sq.connect(sDatabaseName)\n",
        "sDataVaultDir = Base + \"/88-DV\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "  sDatabaseName = sDataVaultDir + \"/datavault.db\"\n",
        "  conn2=sq.connect(sDatabaseName)\n",
        "sFileName = Base + \"/\" + Company + \"/\" + InputDir + \"/\" + InputFileName\n",
        "print(\"###########\")\n",
        "print(\"Loading :\", sFileName)\n",
        "VehicleRaw=pd.read_csv(sFileName, header=0, low_memory=False,encoding=\"latin-1\")\n",
        "sTable = \"Process_Vehicles\"\n",
        "print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "VehicleRaw.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "VehicleRawKey = VehicleRaw[[\"Make\", \"Model\"]].copy()\n",
        "VehicleKey = VehicleRawKey.drop_duplicates()\n",
        "VehicleKey[\"ObjectKey\"] = VehicleKey.apply(lambda row: str(\"(\"+ str(row[\"Make\"]).strip().replace(\" \", \"-\").replace(\"/\", \"-\").lower()+ \")-(\"+ (str(row[\"Model\"]).strip().replace(\" \", \"-\").replace(\" \", \"-\").lower())+ \")\"),axis=1,)\n",
        "VehicleKey[\"ObjectType\"] = VehicleKey.apply(lambda row: \"vehicle\",\n",
        "axis=1)\n",
        "VehicleKey[\"ObjectUUID\"] = VehicleKey.apply(lambda row:\n",
        "str(uuid.uuid4()), axis=1)\n",
        "### Vehicle Hub\n",
        "#\n",
        "VehicleHub = VehicleKey[[\"ObjectType\", \"ObjectKey\",\n",
        "\"ObjectUUID\"]].copy()\n",
        "VehicleHub.index.name = \"ObjectHubID\"\n",
        "sTable = \"Hub-Object-Vehicle\"\n",
        "print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "VehicleHub.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "### Vehicle Satellite\n",
        "#\n",
        "VehicleSatellite = VehicleKey[\n",
        "[\"ObjectType\", \"ObjectKey\", \"ObjectUUID\", \"Make\", \"Model\"]\n",
        "].copy()\n",
        "VehicleSatellite.index.name = \"ObjectSatelliteID\"\n",
        "sTable = \"Satellite-Object-Make-Model\"\n",
        "print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "VehicleSatellite.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "### Vehicle Dimension\n",
        "sView = \"Dim-Object\"\n",
        "print(\"Storing :\", sDatabaseName, \" View:\", sView)\n",
        "sSQL = \"CREATE VIEW IF NOT EXISTS [\" + sView + \"] AS\"\n",
        "sSQL = sSQL + \" SELECT DISTINCT\"\n",
        "sSQL = sSQL + \" H.ObjectType,\"\n",
        "sSQL = sSQL + \" H.ObjectKey AS VehicleKey,\"\n",
        "sSQL = sSQL + \" TRIM(S.Make) AS VehicleMake,\"\n",
        "sSQL = sSQL + \" TRIM(S.Model) AS VehicleModel\"\n",
        "sSQL = sSQL + \" FROM\"\n",
        "sSQL = sSQL + \" [Hub-Object-Vehicle] AS H\"\n",
        "sSQL = sSQL + \" JOIN\"\n",
        "sSQL = sSQL + \" [Satellite-Object-Make-Model] AS S\"\n",
        "sSQL = sSQL + \" ON\"\n",
        "sSQL = sSQL + \" H.ObjectType=S.ObjectType\"\n",
        "sSQL = sSQL + \" AND\"\n",
        "sSQL = sSQL + \" H.ObjectUUID=S.ObjectUUID;\"\n",
        "sql.execute(sSQL, conn2)\n",
        "print(\"Loading :\", sDatabaseName, \" Table:\", sView)\n",
        "sSQL = \" SELECT DISTINCT\"\n",
        "sSQL = sSQL + \" VehicleMake,\"\n",
        "sSQL = sSQL + \" VehicleModel\"\n",
        "sSQL = sSQL + \" FROM\"\n",
        "sSQL = sSQL + \" [\" + sView + \"]\"\n",
        "sSQL = sSQL + \" ORDER BY\"\n",
        "sSQL = sSQL + \" VehicleMake\"\n",
        "sSQL = sSQL + \" AND\"\n",
        "sSQL = sSQL + \" VehicleMake;\"\n",
        "DimObjectData = pd.read_sql_query(sSQL, conn2)\n",
        "DimObjectData.index.name = \"ObjectDimID\"\n",
        "DimObjectData.sort_values([\"VehicleMake\", \"VehicleModel\"],\n",
        "inplace=True, ascending=True)\n",
        "print(DimObjectData)\n",
        "print(\"Vacuum Databases\")\n",
        "sSQL = \"VACUUM;\"\n",
        "sql.execute(sSQL, conn1)\n",
        "sql.execute(sSQL, conn2)\n",
        "conn1.close()\n",
        "conn2.close()\n",
        "print(\"### Don\n"
      ],
      "metadata": {
        "id": "uFobbAfcV7P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6C. Write a program to build relationship between Process-Location and Hub-Location\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3 as sq\n",
        "from pandas.io import sql\n",
        "import uuid\n",
        "\n",
        "Base = \"VKHCG\"\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "\n",
        "Company = \"01-Vermeulen\"\n",
        "InputAssessGraphName = \"Assess_All_Animals.gml\"\n",
        "EDSAssessDir = \"02-Assess/01-EDS\"\n",
        "InputAssessDir = EDSAssessDir + \"/02-Python\"\n",
        "\n",
        "sFileAssessDir = Base + \"/\" + Company + \"/\" + InputAssessDir\n",
        "if not os.path.exists(sFileAssessDir):\n",
        "  os.makedirs(sFileAssessDir)\n",
        "\n",
        "sDataBaseDir = Base + \"/\" + Company + \"/03-Process/SQLite\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "\n",
        "sDatabaseName = sDataBaseDir + \"/Vermeulen.db\"\n",
        "conn1 = sq.connect(sDatabaseName)\n",
        "\n",
        "sDataVaultDir = Base + \"/88-DV\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "\n",
        "sDatabaseName = sDataVaultDir + \"/datavault.db\"\n",
        "conn2 = sq.connect(sDatabaseName)\n",
        "t = 0\n",
        "tMax = 360 * 180\n",
        "\n",
        "for Longitude in range(-180, 180, 10):\n",
        "  for Latitude in range(-90, 90, 10):\n",
        "    t += 1\n",
        "    IDNumber = str(uuid.uuid4())\n",
        "    LocationName = (\"L\"+ format(round(Longitude, 3) * 1000, \"+07d\")+ \"-\"+ format(round(Latitude, 3) * 1000, \"+07d\"))\n",
        "    print(\"Create:\", t, \" of \", tMax, \":\", LocationName)\n",
        "    LocationLine = [(\"ObjectBaseKey\", [\"GPS\"]),(\"IDNumber\", [IDNumber]),(\"LocationNumber\", [str(t)]),(\"LocationName\", [LocationName]),(\"Longitude\", [Longitude]),(\"Latitude\", [Latitude]),]\n",
        "  if t == 1:\n",
        "       LocationFrame = pd.DataFrame.from_dict(dict(LocationLine))\n",
        "  else: \n",
        "    LocationRow=pd.DataFrame.from_dict(dict(LocationLine))\n",
        "    LocationFrame=LocationFrame.append(LocationRow)\n",
        "    LocationHubIndex=LocationFrame.set_index([\"IDNumber\"],inplace=False)\n",
        "\n",
        "sTable = \"Process-Location\"\n",
        "print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "LocationHubIndex.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "\n",
        "sTable = \"Hub-Location\"\n",
        "print(\"Storing :\", sDatabaseName, \" Table:\", sTable)\n",
        "LocationHubIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "\n",
        "print(\"################\")\n",
        "print(\"Vacuum Databases\")\n",
        "sSQL = \"VACUUM;\"\n",
        "sql.execute(sSQL, conn1)\n",
        "sql.execute(sSQL, conn2)\n"
      ],
      "metadata": {
        "id": "QbHb83qzWciO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical No: 07\n",
        "\n",
        "#7A. Write a program to perform Transform Superstep on given data\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import pandas as pd\n",
        "import sqlite3 as sq\n",
        "import uuid\n",
        "pd.options.mode.chained_assignment = None\n",
        "################################################################\n",
        "Base = \"VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "Company = \"01-Vermeulen\"\n",
        "InputDir = \"00-RawData\"\n",
        "InputFileName = \"VehicleData.csv\"\n",
        "################################################################\n",
        "sDataBaseDir = Base + \"/\" + Company + \"/04-Transform/SQLite\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "################################################################\n",
        "sDatabaseName = sDataBaseDir + \"/Vermeulen.db\"\n",
        "conn1 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "sDataVaultDir = Base + \"/88-DV\"\n",
        "if not os.path.exists(sDataVaultDir):\n",
        "  os.makedirs(sDataVaultDir)\n",
        "################################################################\n",
        "sDatabaseName = sDataVaultDir + \"/datavault.db\"\n",
        "conn2 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "sDataWarehouseDir = Base + \"/99-DW\"\n",
        "if not os.path.exists(sDataWarehouseDir):\n",
        "  os.makedirs(sDataWarehouseDir)\n",
        "################################################################\n",
        "sDatabaseName = sDataWarehouseDir + \"/datawarehouse.db\"\n",
        "conn3 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "print(\"\\n#################################\")\n",
        "print(\"Time Category\")\n",
        "print(\"UTC Time\")\n",
        "BirthDateUTC = datetime(1960, 12, 20, 10, 15, 0)\n",
        "BirthDateZoneUTC = BirthDateUTC.replace(tzinfo=timezone(\"UTC\"))\n",
        "BirthDateZoneStr = BirthDateZoneUTC.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "BirthDateZoneUTCStr = BirthDateZoneUTC.strftime(\"%Y-%m-%d %H:%M:%S(%Z)(%z)\")\n",
        "print(BirthDateZoneUTCStr)\n",
        "print(\"#################################\")\n",
        "print(\"Birth Date in Reykjavik :\")\n",
        "BirthZone = \"Atlantic/Reykjavik\"\n",
        "BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone))\n",
        "BirthDateStr = BirthDate.strftime(\"%Y-%m-%d %H:%M:%S (%Z) (%z)\")\n",
        "BirthDateLocal = BirthDate.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(BirthDateStr)\n",
        "print(\"#################################\")\n",
        "################################################################\n",
        "IDZoneNumber = str(uuid.uuid4())\n",
        "sDateTimeKey = BirthDateZoneStr.replace(\" \", \"-\").replace(\":\", \"-\")\n",
        "TimeLine = [\n",
        "(\"ZoneBaseKey\", [\"UTC\"]),\n",
        "(\"IDNumber\", [IDZoneNumber]),\n",
        "(\"DateTimeKey\", [sDateTimeKey]),\n",
        "(\"UTCDateTimeValue\", [BirthDateZoneUTC]),\n",
        "(\"Zone\", [BirthZone]),\n",
        "(\"DateTimeValue\", [BirthDateStr]),\n",
        "]\n",
        "TimeFrame = pd.DataFrame.from_dict(dict(TimeLine))\n",
        "################################################################\n",
        "TimeHub = TimeFrame[[\"IDNumber\", \"ZoneBaseKey\", \"DateTimeKey\",\n",
        "\"DateTimeValue\"]]\n",
        "TimeHubIndex = TimeHub.set_index([\"IDNumber\"], inplace=False)\n",
        "################################################################\n",
        "sTable = \"Hub-Time-Gunnarsson\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "TimeHubIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "sTable = \"Dim-Time-Gunnarsson\"\n",
        "TimeHubIndex.to_sql(sTable, conn3, if_exists=\"replace\")\n",
        "################################################################\n",
        "TimeSatellite = TimeFrame[[\"IDNumber\", \"DateTimeKey\", \"Zone\",\n",
        "\"DateTimeValue\"]]\n",
        "TimeSatelliteIndex = TimeSatellite.set_index([\"IDNumber\"],\n",
        "inplace=False)\n",
        "\n",
        "################################################################\n",
        "BirthZoneFix = BirthZone.replace(\" \", \"-\").replace(\"/\", \"-\")\n",
        "sTable = \"Satellite-Time-\" + BirthZoneFix + \"-Gunnarsson\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "TimeSatelliteIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "sTable = \"Dim-Time-\" + BirthZoneFix + \"-Gunnarsson\"\n",
        "TimeSatelliteIndex.to_sql(sTable, conn3, if_exists=\"replace\")\n",
        "################################################################\n",
        "print(\"\\n#################################\")\n",
        "print(\"Person Category\")\n",
        "FirstName = \"Gumundur\"\n",
        "LastName = \"Gunnarsson\"\n",
        "print(\"Name:\", FirstName, LastName)\n",
        "print(\"Birth Date:\", BirthDateLocal)\n",
        "print(\"Birth Zone:\", BirthZone)\n",
        "print(\"UTC Birth Date:\", BirthDateZoneStr)\n",
        "print(\"#################################\")\n",
        "###############################################################\n",
        "IDPersonNumber = str(uuid.uuid4())\n",
        "PersonLine = [\n",
        "(\"IDNumber\", [IDPersonNumber]),\n",
        "(\"FirstName\", [FirstName]),\n",
        "(\"LastName\", [LastName]),\n",
        "(\"Zone\", [\"UTC\"]),\n",
        "(\"DateTimeValue\", [BirthDateZoneStr]),\n",
        "]\n",
        "PersonFrame = pd.DataFrame.from_dict(dict(PersonLine))\n",
        "################################################################\n",
        "TimeHub = PersonFrame\n",
        "TimeHubIndex = TimeHub.set_index([\"IDNumber\"], inplace=False)\n",
        "################################################################\n",
        "sTable = \"Hub-Person-Gunnarsson\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "TimeHubIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "sTable = \"Dim-Person-Gunnarsson\"\n",
        "TimeHubIndex.to_sql(sTable, conn3, if_exists=\"replace\")\n"
      ],
      "metadata": {
        "id": "iDUrP4-pXiGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7B. Write a program to build dimension Person, dimension Time, and factPersonBornAtTime\n",
        "\n",
        "import os\n",
        "import sqlite3 as sq\n",
        "import sys\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pytz import timezone\n",
        "pd.options.mode.chained_assignment = None\n",
        "################################################################\n",
        "Base = \"VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "Company = \"01-Vermeulen\"\n",
        "################################################################\n",
        "sDataBaseDir = Base + \"/\" + Company + \"/04-Transform/SQLite\"\n",
        "if not os.path.exists(sDataBaseDir):\n",
        "  os.makedirs(sDataBaseDir)\n",
        "################################################################\n",
        "sDatabaseName = sDataBaseDir + \"/Vermeulen.db\"\n",
        "conn1 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "sDataWarehousetDir = Base + \"/99-DW\"\n",
        "if not os.path.exists(sDataWarehousetDir):\n",
        "  os.makedirs(sDataWarehousetDir)\n",
        "################################################################\n",
        "sDatabaseName = sDataWarehousetDir + \"/datawarehouse.db\"\n",
        "conn2 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "print(\"\\n#################################\")\n",
        "print(\"Time Dimension\")\n",
        "BirthZone = \"Atlantic/Reykjavik\"\n",
        "BirthDateUTC = datetime(1960, 12, 20, 10, 15, 0)\n",
        "BirthDateZoneUTC = BirthDateUTC.replace(tzinfo=timezone(\"UTC\"))\n",
        "BirthDateZoneStr = BirthDateZoneUTC.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "BirthDateZoneUTCStr = BirthDateZoneUTC.strftime(\"%Y-%m-%d %H:%M:%S(%Z)(%z)\")\n",
        "BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone))\n",
        "BirthDateStr = BirthDate.strftime(\"%Y-%m-%d %H:%M:%S (%Z) (%z)\")\n",
        "BirthDateLocal = BirthDate.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "################################################################\n",
        "IDTimeNumber = str(uuid.uuid4())\n",
        "TimeLine = [\n",
        "(\"TimeID\", [IDTimeNumber]),\n",
        "(\"UTCDate\", [BirthDateZoneStr]),\n",
        "(\"LocalTime\", [BirthDateLocal]),\n",
        "(\"TimeZone\", [BirthZone]),\n",
        "]\n",
        "TimeFrame = pd.DataFrame.from_dict(dict(TimeLine))\n",
        "################################################################\n",
        "DimTime = TimeFrame\n",
        "DimTimeIndex = DimTime.set_index([\"TimeID\"], inplace=False)\n",
        "################################################################\n",
        "sTable = \"Dim-Time\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "DimTimeIndex.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "DimTimeIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "################################################################\n",
        "print(\"\\n#################################\")\n",
        "print(\"Dimension Person\")\n",
        "print(\"\\n#################################\")\n",
        "FirstName = \"Gumundur\"\n",
        "LastName = \"Gunnarsson\"\n",
        "###############################################################\n",
        "IDPersonNumber = str(uuid.uuid4())\n",
        "PersonLine = [\n",
        "(\"PersonID\", [IDPersonNumber]),\n",
        "(\"FirstName\", [FirstName]),\n",
        "(\"LastName\", [LastName]),\n",
        "(\"Zone\", [\"UTC\"]),\n",
        "(\"DateTimeValue\", [BirthDateZoneStr]),\n",
        "]\n",
        "PersonFrame = pd.DataFrame.from_dict(dict(PersonLine))\n",
        "################################################################\n",
        "DimPerson = PersonFrame\n",
        "DimPersonIndex = DimPerson.set_index([\"PersonID\"], inplace=False)\n",
        "################################################################\n",
        "sTable = \"Dim-Person\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "DimPersonIndex.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "DimPersonIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "################################################################\n",
        "print(\"\\n#################################\")\n",
        "print(\"Fact - Person - time\")\n",
        "print(\"\\n#################################\")\n",
        "IDFactNumber = str(uuid.uuid4())\n",
        "PersonTimeLine = [\n",
        "(\"IDNumber\", [IDFactNumber]),\n",
        "(\"IDPersonNumber\", [IDPersonNumber]),\n",
        "(\"IDTimeNumber\", [IDTimeNumber]),\n",
        "]\n",
        "PersonTimeFrame = pd.DataFrame.from_dict(dict(PersonTimeLine))\n",
        "################################################################\n",
        "FctPersonTime = PersonTimeFrame\n",
        "FctPersonTimeIndex = FctPersonTime.set_index([\"IDNumber\"],\n",
        "inplace=False)\n",
        "################################################################\n",
        "sTable = \"Fact-Person-Time\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "FctPersonTimeIndex.to_sql(sTable, conn1, if_exists=\"replace\")\n",
        "FctPersonTimeIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "################################################################\n"
      ],
      "metadata": {
        "id": "NduV_J1BXr1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical No: 08\n",
        "#8A. Write a program to perform horizontal-style slicing orsubsetting of the data warehouse\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3 as sq\n",
        "################################################################\n",
        "Base = \"VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "################################################################\n",
        "Company = \"01-Vermeulen\"\n",
        "################################################################\n",
        "sDataWarehouseDir = Base + \"/99-DW\"\n",
        "if not os.path.exists(sDataWarehouseDir):\n",
        "  os.makedirs(sDataWarehouseDir)\n",
        "################################################################\n",
        "sDatabaseName = sDataWarehouseDir + \"/datawarehouse.db\"\n",
        "conn1 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "sDatabaseName = sDataWarehouseDir + \"/datamart.db\"\n",
        "conn2 = sq.connect(sDatabaseName)\n",
        "################################################################\n",
        "print(\"################\")\n",
        "sTable = \"Dim-BMI\"\n",
        "print(\"Loading :\", sDatabaseName, \" Table:\", sTable)\n",
        "sSQL = \"SELECT * FROM [Dim-BMI];\"\n",
        "PersonFrame0 = pd.read_sql_query(sSQL, conn1)\n",
        "print(\"################\")\n",
        "sTable = \"Dim-BMI\"\n",
        "print(\"Loading :\", sDatabaseName, \" Table:\", sTable)\n",
        "sSQL = \"SELECT PersonID, \\Height,\\Weight,\\bmi,\\Indicator\\FROM [Dim-BMI]\\WHERE \\Height > 1.5 \\and Indicator = 1 \\ORDER BY \\Height,\\Weight;\"\n",
        "\n",
        "################################################################\n",
        "################################################################\n",
        "sTable = \"Dim-BMI\"\n",
        "print(\"\\n#################################\")\n",
        "print(\"Storing :\", sDatabaseName, \"\\n Table:\", sTable)\n",
        "print(\"\\n#################################\")\n",
        "# DimPersonIndex.to_sql(sTable, conn2, if_exists=\"replace\")\n",
        "################################################################\n",
        "print(\"################\")\n",
        "sTable = \"Dim-BMI\"\n",
        "print(\"Loading :\", sDatabaseName, \" Table:\", sTable)\n",
        "sSQL = \"SELECT * FROM [Dim-BMI];\"\n",
        "PersonFrame2 = pd.read_sql_query(sSQL, conn2)\n",
        "print(\"Full Data Set (Rows):\", PersonFrame0.shape[0])\n",
        "print(\"Full Data Set (Columns):\", PersonFrame0.shape[1])\n",
        "print(\"Horizontal Data Set (Rows):\", PersonFrame2.shape[0])\n",
        "print(\"Horizontal Data Set (Columns):\", PersonFrame2.shape[1])\n"
      ],
      "metadata": {
        "id": "vVGgsMGoXzhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8B. Write a program to perform association rule mining on the given data\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "################################################################\n",
        "Base = \"VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "Company = \"01-Vermeulen\"\n",
        "InputFileName = \"Online-Retail-Billboard.xlsx\"\n",
        "EDSAssessDir = \"02-Assess/01-EDS\"\n",
        "InputAssessDir = EDSAssessDir + \"/02-Python\"\n",
        "################################################################\n",
        "sFileAssessDir = Base + \"/\" + Company + \"/\" + InputAssessDir\n",
        "if not os.path.exists(sFileAssessDir):\n",
        "  os.makedirs(sFileAssessDir)\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/00-RawData/\" + InputFileName\n",
        "################################################################\n",
        "df = pd.read_excel(sFileName)\n",
        "print(df.shape)\n",
        "################################################################\n",
        "df[\"Description\"] = df[\"Description\"].str.strip()\n",
        "df.dropna(axis=0, subset=[\"InvoiceNo\"], inplace=True)\n",
        "df[\"InvoiceNo\"] = df[\"InvoiceNo\"].astype(\"str\")\n",
        "df = df[~df[\"InvoiceNo\"].str.contains(\"C\")]\n",
        "basket = (\n",
        "df[df[\"Country\"] == \"France\"]\n",
        ".groupby([\"InvoiceNo\", \"Description\"])[\"Quantity\"]\n",
        ".sum()\n",
        ".unstack()\n",
        ".reset_index()\n",
        ".fillna(0)\n",
        ".set_index(\"InvoiceNo\"))\n",
        "################################################################\n",
        "def encode_units(x):\n",
        "  if x <= 0:\n",
        "    return 0\n",
        "  if x >= 1:\n",
        "    return 1\n",
        "################################################################\n",
        "basket_sets = basket.applymap(encode_units)\n",
        "basket_sets.drop(\"POSTAGE\", inplace=True, axis=1)\n",
        "frequent_itemsets = apriori(basket_sets, min_support=0.07,\n",
        "use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\",\n",
        "min_threshold=1)\n",
        "print(rules.head())\n",
        "rules[(rules[\"lift\"] >= 6) & (rules[\"confidence\"] >= 0.8)]\n",
        "################################################################\n",
        "sProduct1 = \"ALARM CLOCK BAKELIKE GREEN\"\n",
        "print(sProduct1)\n",
        "print(basket[sProduct1].sum())\n",
        "sProduct2 = \"ALARM CLOCK BAKELIKE RED\"\n",
        "print(sProduct2)\n",
        "print(basket[sProduct2].sum())\n",
        "################################################################\n",
        "basket2 = (df[df[\"Country\"] == \"Germany\"]\n",
        ".groupby([\"InvoiceNo\", \"Description\"])[\"Quantity\"]\n",
        ".sum()\n",
        ".unstack()\n",
        ".reset_index()\n",
        ".fillna(0)\n",
        ".set_index(\"InvoiceNo\"))\n",
        "basket_sets2 = basket2.applymap(encode_units)\n",
        "basket_sets2.drop(\"POSTAGE\", inplace=True, axis=1)\n",
        "frequent_itemsets2 = apriori(basket_sets2, min_support=0.05,\n",
        "use_colnames=True)\n",
        "rules2 = association_rules(frequent_itemsets2, metric=\"lift\",\n",
        "min_threshold=1)\n",
        "print(rules2[(rules2[\"lift\"] >= 4) & (rules2[\"confidence\"] >= 0.5)])\n"
      ],
      "metadata": {
        "id": "EnGtoQcmYCLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8C. Write a program to create a Network Routing Diagram using given data.\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "################################################################\n",
        "pd.options.mode.chained_assignment = None\n",
        "################################################################\n",
        "Base = \"VKHCG\"\n",
        "################################################################\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "sInputFileName = \"02-Assess/01-EDS/02-Python/Assess-Network-Routing-Company.csv\"\n",
        "################################################################\n",
        "sOutputFileName1 = \"05-Organise/01-EDS/02-Python/Organise-Network-Routing-Company.gml\"\n",
        "sOutputFileName2 = \"05-Organise/01-EDS/02-Python/Organise-Network-Routing-Company.png\"\n",
        "Company = \"01-Vermeulen\"\n",
        "################################################################\n",
        "################################################################\n",
        "### Import Country Data\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sInputFileName\n",
        "print(\"################################\")\n",
        "print(\"Loading :\", sFileName)\n",
        "print(\"################################\")\n",
        "CompanyData = pd.read_csv(sFileName, header=0, low_memory=False,\n",
        "encoding=\"latin-1\")\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "print(CompanyData.head())\n",
        "print(CompanyData.shape)\n",
        "################################################################\n",
        "G = nx.Graph()\n",
        "for i in range(CompanyData.shape[0]):\n",
        "  for j in range(CompanyData.shape[0]):\n",
        "    Node0 = CompanyData[\"Company_Country_Name\"][i]\n",
        "    Node1 = CompanyData[\"Company_Country_Name\"][j]\n",
        "\n",
        "if Node0 != Node1:\n",
        "  G.add_edge(Node0, Node1)\n",
        "for i in range(CompanyData.shape[0]):\n",
        "  Node0 = CompanyData[\"Company_Country_Name\"][i]\n",
        "  Node1 = (\n",
        "  CompanyData[\"Company_Place_Name\"][i]\n",
        "  + \"(\"\n",
        "  + CompanyData[\"Company_Country_Name\"][i]\n",
        "  + \")\"\n",
        "  )\n",
        "if Node0 != Node1:\n",
        "  G.add_edge(Node0, Node1)\n",
        "print(\"Nodes:\", G.number_of_nodes())\n",
        "print(\"Edges:\", G.number_of_edges())\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sOutputFileName1\n",
        "print(\"################################\")\n",
        "print(\"Storing :\", sFileName)\n",
        "print(\"################################\")\n",
        "nx.write_gml(G, sFileName)\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sOutputFileName2\n",
        "print(\"################################\")\n",
        "print(\"Storing Graph Image:\", sFileName)\n",
        "print(\"################################\")\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spectral_layout(G, dim=2)\n",
        "nx.draw_networkx_nodes(G, pos, node_color=\"k\", node_size=10,\n",
        "alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, edge_color=\"r\", arrows=False,\n",
        "style=\"dashed\")\n",
        "nx.draw_networkx_labels(G, pos, font_size=12, font_family=\"sans-serif\", font_color=\"b\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(sFileName, dpi=600)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dgws0o7eYJzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical No: 09\n",
        "#Aim: Generating Data\n",
        "#9A. Write a program to perform Report Superstep on Vermeulen PLC\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "################################################################\n",
        "pd.options.mode.chained_assignment = None\n",
        "################################################################\n",
        "Base = \"D:/VKHCG\"\n",
        "################################################################\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "sInputFileName = \"02-Assess/01-EDS/02-Python/Assess-Network-Routing-\n",
        "Customer.csv\"\n",
        "################################################################\n",
        "sOutputFileName1 = \"06-Report/01-EDS/02-Python/Report-Network-Routing-\n",
        "Customer.gml\"\n",
        "sOutputFileName2 = \"06-Report/01-EDS/02-Python/Report-Network-Routing-\n",
        "Customer.png\"\n",
        "Company = \"01-Vermeulen\"\n",
        "################################################################\n",
        "################################################################\n",
        "### Import Country Data\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sInputFileName\n",
        "print(\"################################\")\n",
        "print(\"Loading :\", sFileName)\n",
        "print(\"################################\")\n",
        "CustomerDataRaw = pd.read_csv(sFileName, header=0, low_memory=False,\n",
        "encoding=\"latin-1\")\n",
        "CustomerData = CustomerDataRaw.head(100)\n",
        "print(\"Loaded Country:\", CustomerData.columns.values)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "print(CustomerData.head())\n",
        "################################################################\n",
        "G = nx.Graph()\n",
        "for i in range(CustomerData.shape[0]):\n",
        "for j in range(CustomerData.shape[0]):\n",
        "Node0 = CustomerData[\"Customer_Country_Name\"][i]\n",
        "Node1 = CustomerData[\"Customer_Country_Name\"][j]\n",
        "if Node0 != Node1:\n",
        "G.add_edge(Node0, Node1)\n",
        "for i in range(CustomerData.shape[0]):\n",
        "Node0 = CustomerData[\"Customer_Country_Name\"][i]\n",
        "Node1 = (\n",
        "CustomerData[\"Customer_Place_Name\"][i]\n",
        "+ \"(\"\n",
        "+ CustomerData[\"Customer_Country_Name\"][i]\n",
        "+ \")\"\n",
        ")\n",
        "Node2 = (\n",
        "\"(\"\n",
        "+ \"{:.9f}\".format(CustomerData[\"Customer_Latitude\"][i])\n",
        "+ \")\\\n",
        "(\"\n",
        "+ \"{:.9f}\".format(CustomerData[\"Customer_Longitude\"][i])\n",
        "+ \")\"\n",
        ")\n",
        "if Node0 != Node1:\n",
        "G.add_edge(Node0, Node1)\n",
        "if Node1 != Node2:\n",
        "G.add_edge(Node1, Node2)\n",
        "print(\"Nodes:\", G.number_of_nodes())\n",
        "print(\"Edges:\", G.number_of_edges())\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sOutputFileName1\n",
        "print(\"################################\")\n",
        "print(\"Storing :\", sFileName)\n",
        "print(\"################################\")\n",
        "nx.write_gml(G, sFileName)\n",
        "################################################################\n",
        "sFileName = Base + \"/\" + Company + \"/\" + sOutputFileName2\n",
        "print(\"################################\")\n",
        "print(\"Storing Graph Image:\", sFileName)\n",
        "print(\"################################\")\n",
        "plt.figure(figsize=(25, 25))\n",
        "pos = nx.spectral_layout(G, dim=2)\n",
        "nx.draw_networkx_nodes(G, pos, node_color=\"k\", node_size=10,\n",
        "alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, edge_color=\"r\", arrows=False,\n",
        "style=\"dashed\")\n",
        "\n",
        "nx.draw_networkx_labels(G, pos, font_size=12, font_family=\"sans-\n",
        "serif\", font_color=\"b\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(sFileName, dpi=600)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jcgzs16QYT0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9B. Write a program for Hillman Ltd to convert all numbers on thesides of containers into digits.\n",
        "\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import offsetbox\n",
        "from sklearn import (datasets, decomposition, discriminant_analysis,\n",
        "ensemble,\n",
        "manifold, random_projection)\n",
        "digits = datasets.load_digits(n_class=6)\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "n_samples, n_features = X.shape\n",
        "n_neighbors = 30\n",
        "def plot_embedding(X, title=None):\n",
        "x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "X = (X - x_min) / (x_max - x_min)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = plt.subplot(111)\n",
        "for i in range(X.shape[0]):\n",
        "plt.text(\n",
        "X[i, 0],\n",
        "X[i, 1],\n",
        "str(digits.target[i]),\n",
        "color=plt.cm.Set1(y[i] / 10.0),\n",
        "fontdict={\"weight\": \"bold\", \"size\": 9},\n",
        ")\n",
        "if hasattr(offsetbox, \"AnnotationBbox\"):\n",
        "# only print thumbnails with matplotlib > 1.0\n",
        "shown_images = np.array([[1.0, 1.0]]) # just something big\n",
        "for i in range(digits.data.shape[0]):\n",
        "dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
        "if np.min(dist) < 4e-3:\n",
        "# don't show points that are too close\n",
        "continue\n",
        "shown_images = np.r_[shown_images, [X[i]]]\n",
        "imagebox = offsetbox.AnnotationBbox(\n",
        "offsetbox.OffsetImage(digits.images[i],\n",
        "cmap=plt.cm.gray_r), X[i]\n",
        ")\n",
        "ax.add_artist(imagebox)\n",
        "plt.xticks([]), plt.yticks([])\n",
        "if title is not None:\n",
        "plt.title(title)\n",
        "\n",
        "n_img_per_row = 20\n",
        "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
        "for i in range(n_img_per_row):\n",
        "ix = 10 * i + 1\n",
        "for j in range(n_img_per_row):\n",
        "iy = 10 * j + 1\n",
        "img[ix : ix + 8, iy : iy + 8] = X[i * n_img_per_row +\n",
        "j].reshape((8, 8))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img, cmap=plt.cm.binary)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title(\"A selection from the 64-dimensional digits dataset\")\n",
        "print(\"Computing random projection\")\n",
        "rp = random_projection.SparseRandomProjection(n_components=2,\n",
        "random_state=42)\n",
        "X_projected = rp.fit_transform(X)\n",
        "plot_embedding(X_projected, \"Random Projection of the digits\")\n",
        "print(\"Computing PCA projection\")\n",
        "t0 = time()\n",
        "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\n",
        "plot_embedding(\n",
        "X_pca, \"Principal Components projection of the digits (time\n",
        "%.2fs)\" % (time() - t0)\n",
        ")\n",
        "print(\"Computing Linear Discriminant Analysis projection\")\n",
        "X2 = X.copy()\n",
        "X2.flat[:: X.shape[1] + 1] += 0.01 # Make X invertible\n",
        "t0 = time()\n",
        "X_lda =\n",
        "discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_t\n",
        "ransform(\n",
        "X2, y\n",
        ")\n",
        "plot_embedding(\n",
        "X_lda, \"Linear Discriminant projection of the digits (time %.2fs)\"\n",
        "% (time() - t0)\n",
        ")\n",
        "print(\"Computing Isomap embedding\")\n",
        "t0 = time()\n",
        "X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
        "print(\"Done.\")\n",
        "plot_embedding(X_iso, \"Isomap projection of the digits (time %.2fs)\" %\n",
        "(time() - t0))\n",
        "print(\"Computing LLE embedding\")\n",
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
        "method=\"standard\")\n",
        "t0 = time()\n",
        "X_lle = clf.fit_transform(X)\n",
        "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
        "plot_embedding(\n",
        "X_lle, \"Locally Linear Embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0)\n",
        ")\n",
        "print(\"Computing modified LLE embedding\")\n",
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
        "method=\"modified\")\n",
        "t0 = time()\n",
        "X_mlle = clf.fit_transform(X)\n",
        "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
        "plot_embedding(\n",
        "X_mlle,\n",
        "\"Modified Locally Linear Embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0),\n",
        ")\n",
        "print(\"Computing Hessian LLE embedding\")\n",
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
        "method=\"hessian\")\n",
        "t0 = time()\n",
        "X_hlle = clf.fit_transform(X)\n",
        "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
        "plot_embedding(\n",
        "X_hlle,\n",
        "\"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0),\n",
        ")\n",
        "print(\"Computing LTSA embedding\")\n",
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n",
        "method=\"ltsa\")\n",
        "t0 = time()\n",
        "X_ltsa = clf.fit_transform(X)\n",
        "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
        "plot_embedding(\n",
        "X_ltsa, \"Local Tangent Space Alignment of the digits (time %.2fs)\"\n",
        "% (time() - t0)\n",
        ")\n",
        "print(\"Computing MDS embedding\")\n",
        "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
        "t0 = time()\n",
        "X_mds = clf.fit_transform(X)\n",
        "print(\"Done. Stress: %f\" % clf.stress_)\n",
        "plot_embedding(X_mds, \"MDS embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0))\n",
        "print(\"Computing Totally Random Trees embedding\")\n",
        "hasher = ensemble.RandomTreesEmbedding(n_estimators=200,\n",
        "random_state=0, max_depth=5)\n",
        "t0 = time()\n",
        "X_transformed = hasher.fit_transform(X)\n",
        "pca = decomposition.TruncatedSVD(n_components=2)\n",
        "X_reduced = pca.fit_transform(X_transformed)\n",
        "plot_embedding(\n",
        "X_reduced, \"Random forest embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0)\n",
        ")\n",
        "print(\"Computing Spectral embedding\")\n",
        "embedder = manifold.SpectralEmbedding(\n",
        "n_components=2, random_state=0, eigen_solver=\"arpack\"\n",
        ")\n",
        "t0 = time()\n",
        "X_se = embedder.fit_transform(X)\n",
        "plot_embedding(X_se, \"Spectral embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0))\n",
        "print(\"Computing t-SNE embedding\")\n",
        "tsne = manifold.TSNE(n_components=2, init=\"pca\", random_state=0)\n",
        "t0 = time()\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "plot_embedding(X_tsne, \"t-SNE embedding of the digits (time %.2fs)\" %\n",
        "(time() - t0))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N1U6sGGVYodp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9C. Write a program to perform Kernel Density\n",
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib as ml\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "################################################################\n",
        "Base = \"D:/VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "ml.style.use(\"ggplot\")\n",
        "fig1 = plt.figure(figsize=(10, 10))\n",
        "ser = pd.Series(np.random.randn(1000))\n",
        "ser.plot(figsize=(10, 10), kind=\"kde\")\n",
        "sPicNameOut1 = Base + \"/01-Vermeulen/06-Report/01-EDS/02-\n",
        "Python/kde.png\"\n",
        "plt.savefig(sPicNameOut1, dpi=600)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig2 = plt.figure(figsize=(10, 10))\n",
        "from pandas.plotting import scatter_matrix\n",
        "df = pd.DataFrame(\n",
        "np.random.randn(1000, 5), columns=[\"Y2014\", \"Y2015\", \"Y2016\",\n",
        "\"Y2017\", \"Y2018\"]\n",
        ")\n",
        "scatter_matrix(df, alpha=0.2, figsize=(10, 10), diagonal=\"kde\")\n",
        "sPicNameOut2 = Base + \"/01-Vermeulen/06-Report/01-EDS/02-\n",
        "Python/scatter_matrix.png\"\n",
        "plt.savefig(sPicNameOut2, dpi=600)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xLRX2bZEYy5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9D. Write a program to plot Lag Plot, Autocorrelation, and Bootstrap Plot\n",
        "import sys\n",
        "import pandas as pd\n",
        "from matplotlib import style\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "################################################################\n",
        "Base = \"D:/VKHCG\"\n",
        "print(\"################################\")\n",
        "print(\"Working Base :\", Base, \" using \", sys.platform)\n",
        "print(\"################################\")\n",
        "################################################################\n",
        "style.use(\"ggplot\")\n",
        "from pandas.plotting import lag_plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "data = pd.Series(\n",
        "0.1 * np.random.rand(1000)\n",
        "+ 0.9 * np.sin(np.linspace(-99 * np.pi, 99 * np.pi, num=1000))\n",
        ")\n",
        "lag_plot(data)\n",
        "sPicNameOut1 = Base + \"/01-Vermeulen/06-Report/01-EDS/02-\n",
        "Python/lag_plot.png\"\n",
        "plt.savefig(sPicNameOut1, dpi=600)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "data = pd.Series(\n",
        "0.7 * np.random.rand(1000)\n",
        "+ 0.3 * np.sin(np.linspace(-9 * np.pi, 9 * np.pi, num=1000))\n",
        ")\n",
        "autocorrelation_plot(data)\n",
        "sPicNameOut2 = (\n",
        "Base + \"/01-Vermeulen/06-Report/01-EDS/02-\n",
        "Python/autocorrelation_plot.png\"\n",
        ")\n",
        "\n",
        "plt.savefig(sPicNameOut2, dpi=600)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "from pandas.plotting import bootstrap_plot\n",
        "data = pd.Series(np.random.rand(1000))\n",
        "plt.figure(figsize=(10, 10))\n",
        "bootstrap_plot(data, size=50, samples=500, color=\"grey\")\n",
        "sPicNameOut3 = Base + \"/01-Vermeulen/06-Report/01-EDS/02-\n",
        "Python/bootstrap_plot.png\"\n",
        "plt.savefig(sPicNameOut3, dpi=600)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ycOJDMzZY58B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}